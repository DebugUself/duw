
<!DOCTYPE html>
<html>
<head>
  
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">

    <title>怼周刊_v12 &mdash; 怼圈周刊[DebugUself Weekly]</title>

    <link rel="shortcut icon" href="../../img/favicon.ico">
    <link rel="stylesheet" href="../../css/alabaster.css" type="text/css">
    <link rel="stylesheet" href="../../css/alabaster-overrides.css" type="text/css">

    

    
      <script src="../../search/main.js"></script>
    

    

    <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9">

    
  


<!--
190916 ++tongji.baidu
-->
<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?7b96768fb6a5e86ac53eae70bb5fd60b";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>


</head>
<body>

  <div class="document">
    <div class="documentwrapper">
      <div class="bodywrapper">
        <div class="body" role="main">
          
            <h1 id="_v12">怼周刊_v12<a class="headerlink" href="#_v12" title="Permanent link">¶</a></h1>
<p>~ 预定 17.7.1 20:20 发布</p>
<hr />
<p>时差</p>
<pre><code>天圆地方自古曰
其实正好反过来
夜晩却为地自影
时区穿梭有差异
身体老实跟不上
总算归来查log
又是七天暗进步
迎新筹备还缺嘛
再启新四周项目
自怼小圈稳心气
</code></pre>
<hr />
<ul>
<li>主编: <a href="http://du.zoomquiet.io/2014-02/ac0-zq/">大妈</a></li>
<li>责编:<ul>
<li><a href="http://du.zoomquiet.io/2017-04/about-xpgeng/">xpgeng</a></li>
<li><a href="http://du.zoomquiet.io/2017-04/about-sunoonlee/">sunoonlee</a></li>
<li><a href="http://du.zoomquiet.io/2017-04/about-zoe/">Zoe</a></li>
<li><a href="http://du.zoomquiet.io/2017-04/about-bambooom/">bambooom</a></li>
</ul>
</li>
</ul>
<h2 id="_1">进度<a class="headerlink" href="#_1" title="Permanent link">¶</a></h2>
<p>~ 记录当周关键事件日期+证据链接</p>
<ul>
<li>170624 <a href="https://github.com/DebugUself/du4proto/issues/148">72h [ANN] 170624 怼周会及会议纪要</a></li>
<li>170401 关闭报表和入密</li>
<li>170331 om103py 毕业</li>
</ul>
<h2 id="_2">任务<a class="headerlink" href="#_2" title="Permanent link">¶</a></h2>
<p>~ 记述关键共怼任务 (如果没有, 留空)</p>
<ul>
<li>170624 <a href="https://github.com/DebugUself/du4proto/blob/master/S04E51/README.md">S04E51 启动</a></li>
<li>170603 <a href="https://github.com/DebugUself/du4proto/issues/135">怼圈的二次开放 筹备中</a></li>
</ul>
<h2 id="_3">进展<a class="headerlink" href="#_3" title="Permanent link">¶</a></h2>
<p>~ 整体上圈内部活跃指标情况</p>
<ul>
<li>提交(S04E051): 7 人 (6人延续上期 + 1 个新项目)</li>
<li>小组 @zoomquiet 时间帐单:效能分析小队<ul>
<li>@zoomquiet </li>
<li>@zsy</li>
<li>@liguanghe</li>
<li>@simpleowen </li>
<li>@mxclover </li>
</ul>
</li>
<li>@hetao Deep Learning 自学计划</li>
<li>@zoejane 进入 Web 世界</li>
<li>引发的作品:<ul>
<li>@hetao - Deep Learning 学习笔记</li>
<li>@zoejane - Demo 网页</li>
</ul>
</li>
<li>状态:</li>
</ul>
<table>
<tr><th>allcic Commit</th><th> times</th><th>weekly Commit</th><th> times</th></tr>
<tr><td>
            <a href='http://github.com/ZoomQuiet'>ZoomQuiet</a></td><td>255</td>
        <td>
            <a href='http://github.com/zoejane'>zoejane</a></td><td>25</td>

<tr><td>
            <a href='http://github.com/zoejane'>zoejane</a></td><td>239</td>
        <td>
            <a href='http://github.com/mxclover'>mxclover</a></td><td>10</td>

<tr><td>
            <a href='http://github.com/liguanghe'>liguanghe</a></td><td>117</td>
        <td>
            <a href='http://github.com/zhangshiyinrunwithcc'>zhangshiyinrunwithcc</a></td><td>2</td>

<tr><td>
            <a href='http://github.com/mxclover'>mxclover</a></td><td>113</td>
        <td>
            <a href='http://github.com/ZoomQuiet'>ZoomQuiet</a></td><td>1</td>

<tr><td>
            <a href='http://github.com/bambooom'>bambooom</a></td><td>107</td>
        <td>
            <a href='http://github.com/livingworld'>livingworld</a></td><td>1</td>

<tr><th>all Commit </th><th>Comments times</th><th>weekly Commit</th><th>Comments times</th></tr>
<tr><th>all Issue </th><th>Comments times</th><th>weekly Issue</th><th>Comments times</th></tr>
<tr><td>
            <a href='http://github.com/ZoomQuiet'>ZoomQuiet</a></td><td>307</td>
        <td>
            <a href='http://github.com/zhangshiyinrunwithcc'>zhangshiyinrunwithcc</a></td><td>8</td>

<tr><td>
            <a href='http://github.com/zhangshiyinrunwithcc'>zhangshiyinrunwithcc</a></td><td>168</td>
        <td>
            <a href='http://github.com/zoejane'>zoejane</a></td><td>3</td>

<tr><td>
            <a href='http://github.com/liguanghe'>liguanghe</a></td><td>129</td>
        <td>
            <a href='http://github.com/mxclover'>mxclover</a></td><td>1</td>

<tr><td>
            <a href='http://github.com/zoejane'>zoejane</a></td><td>70</td>
        <td>
            <a href='http://github.com/livingworld'>livingworld</a></td><td>1</td>

</table>

<p>&lt;- 170701 16:22</p>
<ul>
<li>在线(测试ing..):<ul>
<li><code>curl du.zoomquiet.us</code></li>
<li><code>curl du.zoomquiet.us/v0/all/cic/rank/5/</code></li>
<li><code>curl du.zoomquiet.us/v0/all/cil/rank/5/</code></li>
<li><code>curl du.zoomquiet.us/v0/week/cic/rank/5/</code></li>
<li><code>curl du.zoomquiet.us/v0/week/cil/rank/5/</code></li>
</ul>
</li>
</ul>
<h2 id="_4">成果<a class="headerlink" href="#_4" title="Permanent link">¶</a></h2>
<p>~ 各种成品/半成品 内部知识作品</p>
<h2 id="hetao-deep-learning">@hetao - Deep Learning学习笔记<a class="headerlink" href="#hetao-deep-learning" title="Permanent link">¶</a></h2>
<ul>
<li><a href="https://github.com/DebugUself/du4proto/tree/hetao">DebugUself/du4proto at hetao</a></li>
<li>@hetao 童鞋近期在 GitHub 更新了自己这几周以来对于 Depp Learning 非常详细的学习笔记,推荐!</li>
</ul>
<h3 id="20170626-skip-gram-word2">20170626 Skip-gram word2 笔记<a class="headerlink" href="#20170626-skip-gram-word2" title="Permanent link">¶</a></h3>
<p>Skip-Gram主要是给定input word来预测上下文,区别于CBOW给定上下文预测input word. </p>
<p>Word2Vec模型实际上分为了两个部分,第一部分为建立模型,第二部分是通过模型获取嵌入词向量. </p>
<h3 id="_5">第一部分:模型<a class="headerlink" href="#_5" title="Permanent link">¶</a></h3>
<h4 id="fake-task">Fake Task<a class="headerlink" href="#fake-task" title="Permanent link">¶</a></h4>
<ul>
<li>印象:训练模型的真正目的是获得模型基于训练数据学得的隐层权重. 为了得到这些权重,我们首先需要构建完整的神经网络作为我们的"Fake Task". </li>
<li>例子:以一个句子<code>"The dog barked at the mailman"</code>的输入为例. <ul>
<li>首选选择,单词"dog"作为input word. </li>
<li>其次,定义一个叫做<strong>skip_window</strong>的参数,它代表着我们从当前input word的一侧(左边或右边)选取词的数量. 如果我们设置<code>skip_window=2</code>,那么我们最终获得窗口中的词(包括input word在内)就是['The', 'dog','barked', 'at']. <code>skip_window=2</code>代表着选取左input word左侧2个词和右侧2个词进入我们的窗口,所以整个窗口大小span=2x2=4. </li>
<li>另一个参数叫<strong>num_skips</strong>,它代表着我们从整个窗口中选取多少个不同的词作为我们的output word,当<code>skip_window=2</code>,<code>num_skips=2</code>时,我们将会得到两组 (input word, output word) 形式的训练数据,即 ('dog', 'barked'),('dog', 'the'). </li>
<li>神经网络基于这些训练数据将会输出一个概率分布,这个概率代表着我们的词典中的每个词是output word的可能性. 例如我们先拿一组数据 ('dog', 'barked') 来训练神经网络,那么模型通过学习这个训练样本,会告诉我们词汇表中每个单词是"barked"的概率大小. </li>
<li>模型的输出概率代表着到我们词典中每个词有多大可能性跟input word同时出现. </li>
<li><strong>window_size=2</strong>表示选择输入词前后各两个词和输入词进行组合,如图蓝色代表input word,方框内代表位于窗口内的单词,由图可知,可以统计词汇组合出现概率高低. <ul>
<li><img alt="" src="https://raw.githubusercontent.com/DebugUself/du4proto/hetao/week8/note/images/window_size.png?token=ABQhvBWcLwFRicNJ1heHDKnP-yo3GPRRks5ZYEqlwA%3D%3D" /></li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="_6">模型细节<a class="headerlink" href="#_6" title="Permanent link">¶</a></h4>
<ul>
<li>首选构建自己的词汇表(vocabulary)再对单词进行one-hot编码. </li>
<li>假设从我们的训练文档中抽取出10000个唯一不重复的单词组成词汇表. 我们对这10000个单词进行one-hot编码,得到的每个单词都是一个10000维的向量,向量每个维度的值只有0或者1,假如单词ants在词汇表中的出现位置为第3个,那么ants的向量就是一个第三维度取值为1,其他维都为0的10000维的向量($ants=[0, 0, 1, 0, ..., 0]$). </li>
<li>还是上面的例子,"The dog barked at the mailman",那么我们基于这个句子,可以构建一个大小为5的词汇表(忽略大小写和标点符号):("the", "dog", "barked", "at", "mailman"),我们对这个词汇表的单词进行编号0-4. 那么"dog"就可以被表示为一个5维向量[0, 1, 0, 0, 0]. </li>
<li>模型的输入如果为一个10000维的向量,那么输出也是一个10000维度(词汇表的大小)的向量,它包含了10000个概率,每一个概率代表着当前词是输入样本中output word的概率大小. </li>
<li><img alt="" src="https://raw.githubusercontent.com/DebugUself/du4proto/hetao/week8/note/images/neural_network.png?token=ABQhvNCC06XQfJwEMwBAZufGtntBrecgks5ZYEq5wA%3D%3D" /></li>
<li>隐层没有使用任何激活函数,但是输出层使用了sotfmax. </li>
<li>我们基于成对的单词来对神经网络进行训练,训练样本是 ( input word, output word ) 这样的单词对,input word和output word都是one-hot编码的向量. 最终模型的输出是一个概率分布. </li>
</ul>
<h4 id="_7">隐层<a class="headerlink" href="#_7" title="Permanent link">¶</a></h4>
<p>用300个特征节点表示一个单词,即用300维向量表示,则隐藏层权重矩阵为10000x300维度. 
看下面的图片,左右两张图分别从不同角度代表了输入层-隐层的权重矩阵. 左图中每一列代表一个10000维的词向量和隐层单个神经元连接的权重向量. 从右边的图来看,每一行实际上代表了每个单词的词向量. </p>
<p>上面我们提到,input word和output word都会被我们进行one-hot编码. 如果我们将一个1 x 10000的向量和10000 x 300的矩阵相乘,它会消耗相当大的计算资源,为了高效计算,它仅仅会选择矩阵中对应的向量中维度值为1的索引行(这句话很绕),看图就明白. </p>
<p>上面的例子中,左边向量中取值为1的对应维度为3(下标从0开始),那么计算结果就是矩阵的第3行(下标从0开始)--- [10, 12, 19],这样模型中的隐层权重矩阵便成了一个"查找表"(lookup table),</p>
<h4 id="_8">输出层<a class="headerlink" href="#_8" title="Permanent link">¶</a></h4>
<p>经过神经网络隐层的计算,ants这个词会从一个1 x 10000的向量变成1 x 300的向量,再被输入到输出层. 输出层是一个softmax回归分类器,它的每个结点将会输出一个0-1之间的值(概率),这些所有输出层神经元结点的概率之和为1. 
下面是一个例子,训练样本为 (input word: "ants", output word: "car") 的计算示意图. 
<img alt="" src="https://raw.githubusercontent.com/DebugUself/du4proto/hetao/week8/note/images/input_output.png?token=ABQhvPeeIv6dMuE67Lo4hj5LYmjB2MnWks5ZYErMwA%3D%3D" /></p>
<h4 id="_9">直觉上的理解<a class="headerlink" href="#_9" title="Permanent link">¶</a></h4>
<p>针对上下文相似的情况,进行词干话(stemming),就是去除词缀得到词根的过程. </p>
<h3 id="skip-gram">第二部分 基于skip-gram模型的高效训练<a class="headerlink" href="#skip-gram" title="Permanent link">¶</a></h3>
<p>针对权重矩阵过大的问题,Word2Vec做出了如下优化:</p>
<ul>
<li>将常见的单词组合(word pairs)或者词组作为单个"words"来处理. </li>
<li>对高频次单词进行抽样来减少训练样本的个数. </li>
<li>对优化目标采用"negative sampling"方法,这样每个训练样本的训练只会更新一小部分的模型权重,从而降低计算负担. </li>
</ul>
<p>这是一个<a href="http://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/">模型词汇表</a>,及<a href="https://github.com/chrisjmccormick/inspect_word2vec/tree/master/vocabulary">vocabulary</a>,相应的论文有<a href="https://arxiv.org/pdf/1310.4546.pdf">Distributed Representations of Words and Phrases
and their Compositionality</a>,<a href="https://code.google.com/archive/p/word2vec/">代码</a>. </p>
<h4 id="_10">对高频词抽样<a class="headerlink" href="#_10" title="Permanent link">¶</a></h4>
<p>如下图所示,对于"the"这种常用高频单词,既无意义,又的比重大. 对于我们在训练原始文本中遇到的每一个单词,它们都有一定概率被我们从文本中删掉,而这个被删除的概率与单词的频率有关. 
<img alt="" src="https://raw.githubusercontent.com/DebugUself/du4proto/hetao/week8/note/images/window_size.png?token=ABQhvLEjfwZz2EuoVM_DN8ZqQCWUNtrZks5ZYErdwA%3D%3D" /></p>
<h4 id="_11">抽样率<a class="headerlink" href="#_11" title="Permanent link">¶</a></h4>
<p>在语料库中,该词出现概率越低,越有可能被保留. $P(w_i)=1-\sqrt{\frac{t}{f(w_i)}}$. 在代码中还有一个参数叫"sample",这个参数代表一个阈值,默认值为0.001,这个值越低,越不容易保留,即$P(w_i)$越大. </p>
<h4 id="negative-sampling">负采样(negative sampling)<a class="headerlink" href="#negative-sampling" title="Permanent link">¶</a></h4>
<ul>
<li>印象:用来提高训练速度并且改善所得到词向量的质量的一种方法. 不同于原本每个训练样本更新所有的权重,负采样每次让一个训练样本仅仅更新一小部分的权重,这样就会降低梯度下降过程中的计算量. </li>
<li>例子:<ul>
<li>当我们用训练样本 ( input word: "fox",output word: "quick") 来训练我们的神经网络时,"fox"和"quick"都是经过one-hot编码的. 如果我们的vocabulary大小为10000时,在输出层,我们期望对应"quick"单词的那个神经元结点输出1,其余9999个都应该输出0. 在这里,这9999个我们期望输出为0的神经元结点所对应的单词我们称为"negative" word. </li>
<li>如果使用了负采样的方法我们仅仅去更新我们的positive word-"quick"的和我们选择的其他5个negative words的结点对应的权重,共计6个输出神经元,相当于每次只更新$300x 6=1800$个权重. </li>
<li>在论文中,作者指出指出对于小规模数据集,选择5-20个negative words会比较好,对于大规模数据集可以仅选择2-5个negative words. </li>
</ul>
</li>
</ul>
<h4 id="negative-words">如何选择negative words<a class="headerlink" href="#negative-words" title="Permanent link">¶</a></h4>
<p>我们使用"一元模型分布(unigram distribution)"来选择"negative words". 要注意的一点是,一个单词被选作negative sample的概率跟它出现的频次有关,出现频次越高的单词越容易被选作negative words. 
代码公式如下:
$P(w_i)=\frac{f(w_i)^{3/4}}{\sum_{j=0}^n(f(w_j)^{3/4})}$
每个单词被赋予一个权重,即$f(w_i)$, 它代表着单词出现的频次. 公式中开3/4的根号完全是基于经验的,论文中提到这个公式的效果要比其它公式更加出色. </p>
<ul>
<li><a href="https://github.com/chrisjmccormick/word2vec_commented">c语言实现代码</a></li>
<li><a href="https://github.com/chrisjmccormick/word2vec_commented">Word2Vec Resources教程</a></li>
</ul>
<h3 id="tensorflowskip-gram">第三部分:基于TensorFlow实现Skip-Gram模型<a class="headerlink" href="#tensorflowskip-gram" title="Permanent link">¶</a></h3>
<p>分为四部分:</p>
<ul>
<li>数据预处理</li>
<li>
<p>训练样本构建</p>
<ul>
<li>采样</li>
<li>构造batch<ul>
<li>找到每个input word的上下文:如果我们固定skip_window=2的话,那么fox的上下文就是[quick, brown, jumps, over]. <ul>
<li>我在实际选择input word上下文时,使用的窗口大小是一个介于[1, window_size]区间的随机数. 这里的目的是让模型更多地去关注离input word更近词. </li>
</ul>
</li>
<li>基于上下文构建batch:如果我们的batch_size=1的话,那么实际上一个batch中有四个训练样本. </li>
</ul>
</li>
</ul>
</li>
<li>
<p>模型构建</p>
<ul>
<li>输入层到嵌入层<ul>
<li>输入层到隐层的权重矩阵作为嵌入层要给定其维度,一般embeding_size设置为50-300之间. </li>
</ul>
</li>
<li>嵌入层到输出层<ul>
<li>TensorFlow中的sampled_softmax_loss,由于进行了negative sampling,所以实际上我们会低估模型的训练loss. <ul>
<li>有点儿费解. </li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>模型验证</li>
</ul>
<h4 id="_12">参考资源<a class="headerlink" href="#_12" title="Permanent link">¶</a></h4>
<ul>
<li><a href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/">A really good conceptual overview of word2vec from Chris McCormick</a></li>
<li><a href="https://arxiv.org/pdf/1301.3781.pdf">First word2vec paper from Mikolov et al.</a></li>
<li><a href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">NIPS paper with improvements for word2vec also from Mikolov et al.</a></li>
<li><a href="http://www.thushv.com/natural_language_processing/word2vec-part-1-nlp-with-deep-learning-with-tensorflow-skip-gram/">An implementation of word2vec from Thushan Ganegedara</a></li>
<li><a href="https://www.tensorflow.org/tutorials/word2vec">TensorFlow word2vec tutorial</a></li>
<li><a href="https://zhuanlan.zhihu.com/zhaoyeyu">知乎专栏</a></li>
</ul>
<h3 id="_13">零碎卡片<a class="headerlink" href="#_13" title="Permanent link">¶</a></h3>
<h4 id="word-embeddings">Word embeddings<a class="headerlink" href="#word-embeddings" title="Permanent link">¶</a></h4>
<ul>
<li>印象:one-hot encode效率太低,浪费计算资源. 为了解决这个问题,采用了所谓embeddings,通过直接从权重矩阵中获取隐藏层值来跳过嵌入层的乘法,即使用权重矩阵作为查找表. </li>
<li>例子:例如"heart"被编码为958,"mind"为18094.然后为了获取"heart"的隐藏层值,只需要嵌入矩阵的第958行. 这个过程称为嵌入式查找,隐藏单元的数量是嵌入维度. <ul>
<li><img alt="" src="https://raw.githubusercontent.com/DebugUself/du4proto/hetao/week8/note/images/lookup_matrix.png?token=ABQhvOms5WWP6LWu8qx0iukDk9CYQ4Ouks5ZYEtMwA%3D%3D" /></li>
<li><img alt="" src="https://raw.githubusercontent.com/DebugUself/du4proto/hetao/week8/note/images/tokenize_lookup.png?token=ABQhvLHc3PWG3nLblV-ZTa1gk5jGDxpPks5ZYEtzwA%3D%3D" /></li>
</ul>
</li>
</ul>
<h4 id="word2vec">Word2Vec<a class="headerlink" href="#word2vec" title="Permanent link">¶</a></h4>
<ul>
<li>印象:两种实现Word2Vec的结构:<ul>
<li>CBOW (Continuous Bag-Of-Words)</li>
<li>Skip-gram
<img alt="" src="https://raw.githubusercontent.com/DebugUself/du4proto/hetao/week8/note/images/word2vec_architectures.png?token=ABQhvNFrrnOwVj9uK9LSkao02U8ZSecoks5ZYEuKwA%3D%3D" /></li>
</ul>
</li>
</ul>
<h4 id="preprocessing">Preprocessing<a class="headerlink" href="#preprocessing" title="Permanent link">¶</a></h4>
<p>印象:根据词频排序,频率最高的为0,其次为1,依次排序. 最终,输入一段words,输出的是一段文字中单词对应的序列位置. </p>
<h4 id="subsampling">Subsampling<a class="headerlink" href="#subsampling" title="Permanent link">¶</a></h4>
<ul>
<li>印象:去除一些无意义的词汇. 公式表达式为:
$P(w_i)=1-\sqrt{\frac{t}{f(w_i)}}$
其中,t是惩罚参数,$f(w_i)$是$w_i$在数据集中的频率,$P(w_i)$是丢弃word的概率</li>
<li>例子:将一些无意义的词汇,例如"the","of"等去除. </li>
</ul>
<h4 id="making-batches">Making batches<a class="headerlink" href="#making-batches" title="Permanent link">¶</a></h4>
<ul>
<li>印象:获取一段话前后的word个数. </li>
<li>例子:</li>
</ul>
<pre><code>def get_target(words, idx, window_size=5):
    ''' Get a list of words in a window around an index. '''
    R = np.random.randint(1, window_size+1)
    start = idx - R if (idx - R) &gt; 0 else 0
    stop = idx + R
    target_words = set(words[start:idx] + words[idx+1:stop+1]) # 切片之后打乱了顺序

    # Your code here
    return list(target_words)
</code></pre>

<h2 id="zoejane-demo">zoejane - Demo 网页<a class="headerlink" href="#zoejane-demo" title="Permanent link">¶</a></h2>
<ul>
<li><a href="https://github.com/DebugUself/du4proto/blob/master/S04E51/du_s04e51_zoejane_plan.md">Project Plan</a></li>
<li><a href="http://blog.zoejane.net/learn-web">Demo 网页</a></li>
</ul>
<h2 id="_14">时间帐单:效能分析小队 推进中<a class="headerlink" href="#_14" title="Permanent link">¶</a></h2>
<ul>
<li>组员<ul>
<li><code>(￣▽￣)</code> -&gt; 大妈</li>
<li>🐻 <code>熊</code> =&gt; @zhangshiyinrunwithcc</li>
<li>🐣 <code>鹤</code> =&gt; @李广鹤</li>
<li>🐈 <code>猫</code> =&gt; @simpleowen</li>
<li>🐴 <code>mx</code> =&gt; @mxclover</li>
</ul>
</li>
<li>目标<ul>
<li>通过分析大妈和剑飞, 两人5年以上持续时间帐单的数据</li>
<li>获得数据化的行为效能结论</li>
<li>对自身行为给出几点优化策略</li>
</ul>
</li>
<li><a href="https://github.com/DebugUself/du4proto/tree/atl4dama">Github 项目链接</a></li>
<li><a href="https://github.com/DebugUself/du4proto/blob/master/S03E51/du_s03e51_zoomquiet_plan.md">Project Plan</a>  </li>
</ul>
<h2 id="_15">故事<a class="headerlink" href="#_15" title="Permanent link">¶</a></h2>
<p>~ 收集各自无法雷同的怼圈真人故事...</p>
<h2 id="_16">推荐<a class="headerlink" href="#_16" title="Permanent link">¶</a></h2>
<p>~ 嗯哼各种怼路上发现的嗯哼...</p>
<h2 id="_17">后记<a class="headerlink" href="#_17" title="Permanent link">¶</a></h2>
<p>~ 怼周刊是什么以及为什么和能怎么...</p>
<p>大妈曰过: <code>参差多态 才是生机</code>
问题在 <code>参差</code> 的行为是无法形成团队的</p>
<pre><code>Coming together is a beginning; 
Keeping together is progress; 
Working together is success!
</code></pre>
<p>&lt;--- <a href="https://www.brainyquote.com/quotes/quotes/h/henryford121997.html">Henry Ford</a></p>
<ul>
<li>所以, 有了 大妈 随见随怼的持续嗯哼...</li>
<li>但是, 想象一年后, 回想几十周前自己作的那些 <code>图样图森破</code> </li>
<li>却没现成的资料来出示给后进来嗯哼?</li>
<li>不科学, 值得记录的, 就应当有个形式固定下来</li>
<li>所以,有了这个 <code>怼周刊</code> (Weekly 4 DU)</li>
</ul>
            

            <!--
            
          -->

            <!--190910 init. comments-->
            <hr/>

<h3>精品小班4Py 按时完成 ;-)</h3>
<h3>101camp5Py 大年初十上线</h3>
<blockquote><p>
    
<ul>
    <li>
    开始报名: <b>2020.2.3</b>
    </li>
    <li>
    报名结束: <b>2020.2.23</b>
    </li>
    <li>
    正式开课: <b>2020.3.2</b>
    </li>
    <li>
    课程结束: <b>2020.4.12</b>
    </li>
</ul>
</p></blockquote>

<hr/>

<h2>蟒营</h2>
<p>
    伴你重享<b>学习</b>乐趣
</p>
<blockquote>
<p>
    <b>
    101.camp
    </b>
</p>
<p>
    <i>
    Reactivate Joy by Self-teach with You
    </i>
</p>
</blockquote>


<br/>
任何问题可先进入知识星球<b>(免费)</b>咨询:
<br/>
<img alt="FAQ" 
    src="https://ipic.zoomquiet.top/2019-08-28-FAQ101camp.jpeg"/>


<p>关注公众号, 持续获得相关各种咨询, :
<img alt="mainium" src="https://ipic.zoomquiet.top/2019-10-12-mainium-qr-barnner.jpg"/>

<!--
<img alt="PythoniCamp" src="http://0.zoomquiet.top/logos/101.camp/banner_101camp_h191.jpg"/>

    <br/>
实在想立即开始感受蟒营, 可以加入知识星球(免费):
    <br/>

<img alt="蟒营101camp" src="https://ipic.zoomquiet.top/2019-08-09-190809camp101.jpeg"/>

-->
</p>

<hr/>

<h3>追问</h3>
<blockquote>
<p>
    任何问题, 随时邮件提问可也:
<br/>
    <i>
    ask@101.camp
    </i>
</p>
</blockquote>

<hr/>





            <!-- comments by gh-issue -->
<script src="https://utteranc.es/client.js"
        repo="DebugUself/duw"
        issue-term="url"
        label="✨💬✨ "
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>

          
        </div>
      </div>
    </div>
    <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
      <div class="sphinxsidebarwrapper">
        
          
            
  <p class="logo">
    <a href="../..">
      <!--
      src="../../https://blog.101.camp/logo.gif" 
      -->
      <img class="logo" 
      src="https://blog.101.camp/logo.gif" 
      title="Ink sketch of onion">
    </a>
  </p>
  



          
            



<h3>Table Of Contents</h3>

<nav>
  
  
    <ul>
    
      <li><a href="#_1">进度</a></li>
      <ul>
    
  </ul>
    
      <li><a href="#_2">任务</a></li>
      <ul>
    
  </ul>
    
      <li><a href="#_3">进展</a></li>
      <ul>
    
  </ul>
    
      <li><a href="#_4">成果</a></li>
      <ul>
    
  </ul>
    
      <li><a href="#hetao-deep-learning">@hetao - Deep Learning学习笔记</a></li>
      <ul>
    
      <li><a href="#20170626-skip-gram-word2">20170626 Skip-gram word2 笔记</a></li>
      <ul>
    
  </ul>
    
      <li><a href="#_5">第一部分:模型</a></li>
      <ul>
    
      <li><a href="#fake-task">Fake Task</a></li>
      <ul>
    
  </ul>
    
      <li><a href="#_6">模型细节</a></li>
      <ul>
    
  </ul>
    
      <li><a href="#_7">隐层</a></li>
      <ul>
    
  </ul>
    
      <li><a href="#_8">输出层</a></li>
      <ul>
    
  </ul>
    
      <li><a href="#_9">直觉上的理解</a></li>
      <ul>
    
  </ul>
    
  </ul>
    
      <li><a href="#skip-gram">第二部分 基于skip-gram模型的高效训练</a></li>
      <ul>
    
      <li><a href="#_10">对高频词抽样</a></li>
      <ul>
    
  </ul>
    
      <li><a href="#_11">抽样率</a></li>
      <ul>
    
  </ul>
    
      <li><a href="#negative-sampling">负采样(negative sampling)</a></li>
      <ul>
    
  </ul>
    
      <li><a href="#negative-words">如何选择negative words</a></li>
      <ul>
    
  </ul>
    
  </ul>
    
      <li><a href="#tensorflowskip-gram">第三部分:基于TensorFlow实现Skip-Gram模型</a></li>
      <ul>
    
      <li><a href="#_12">参考资源</a></li>
      <ul>
    
  </ul>
    
  </ul>
    
      <li><a href="#_13">零碎卡片</a></li>
      <ul>
    
      <li><a href="#word-embeddings">Word embeddings</a></li>
      <ul>
    
  </ul>
    
      <li><a href="#word2vec">Word2Vec</a></li>
      <ul>
    
  </ul>
    
      <li><a href="#preprocessing">Preprocessing</a></li>
      <ul>
    
  </ul>
    
      <li><a href="#subsampling">Subsampling</a></li>
      <ul>
    
  </ul>
    
      <li><a href="#making-batches">Making batches</a></li>
      <ul>
    
  </ul>
    
  </ul>
    
  </ul>
    
      <li><a href="#zoejane-demo">zoejane - Demo 网页</a></li>
      <ul>
    
  </ul>
    
      <li><a href="#_14">时间帐单:效能分析小队 推进中</a></li>
      <ul>
    
  </ul>
    
      <li><a href="#_15">故事</a></li>
      <ul>
    
  </ul>
    
      <li><a href="#_16">推荐</a></li>
      <ul>
    
  </ul>
    
      <li><a href="#_17">后记</a></li>
      <ul>
    
  </ul>
    
  </ul>
  

  
    <hr>
    <ul>
      
      <!--
        <li class="toctree-l1"><a href="https://github.com/DebugUself/duw">Fork me on GitHub</a></li>
      -->
        <li class="toctree-l1"><a href="https://github.com/101camp/blog/tree/master/docs/2017/012w.md">Fork me on GitHub</a></li>

      
    </ul>
  
</nav>
          
            

          
            <!--
<div id="searchbox" style="display: none;" role="search">
  <h3>Quick search</h3>
  <form class="search" action="../../search.html" method="get">
    <input name="q" type="text">
    <input value="Go" type="submit">
  </form>
  <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
  </p>
</div>
<script type="text/javascript">
  document.getElementById("searchbox").style.display = "block";
</script>

-->
          
        

        <!--190910 init. comments-->
        <h3>Years</h3>
<ul>
    <li><a href="2020/">2020 怼周刊</a></li>
    <li><a href="2019/">2019 怼周刊</a></li>
    <li><a href="2018/">2018 怼周刊</a></li>
    <li><a href="2017/">2017 怼周刊</a></li>
</ul>

  <h3>Navigation</h3>


<ul>
  
      
        <li>
          <a href="/duw/">Home</a>
        </li>
      
    
      
        <li>
          <a href="/duw/publish/">Publish</a>
        </li>
      
    
      
        <li>
          <a href="/duw/license/">License</a>
        </li>
      
    
  </ul>

<h3>Powered by</h3>

    <li>
        <a href="https://du.101.camp/">自怼圈</a>
    </li>
    <li>
        <a href="https://du.101.camp/PoDU/">怼力通证</a>
    </li>

<ul>
    <li>
        <a href="https://101.camp/">蟒营™</a>
<ul>
    <li>
        <a href="https://py.101.camp/">Python 入门班</a>
    </li>
    <li>
        <a href="https://io.101.camp/">图谱</a>
    </li>
    <li>
        <a href="https://slides.101.camp/">幻灯</a>
    </li>
    <li>
        <a href="https://wiki.101.camp/">维基</a>
    </li>
    <li>
        <a href="https://github.com/101camp/comments/issues">评注</a>
    </li>
</ul>
    </li>
    <li>
        <a href="http://weekly.pychina.org/">蟒周刊</a>
    </li>
    <li>
        "大妈"们
<ul>
    <li>
        <a href="https://yixuan.li/">Yixuan</a>
    </li>

    <li>
        <a href="https://www.douban.com/people/Azeril/">Azeril</a>
    </li>
    <li>
        <a href="https://zoomquiet.io/">Zoom.Quiet</a>
    </li>
    <li>
        ...
    </li>
</ul>
    <li>
        <a href="http://mainium.icu">昧因科技®</a>
    </li>
    </li>
</ul>

      </div>
    </div>
    <div class="clearer"></div>
  </div>

  
    <div class="footer">
      
        &copy; ©101camp 2017
      
      
        |
        Powered by <a href="http://www.mkdocs.org">mkdocs 1.0.4</a>
        &amp; <a href="https://github.com/iamale/mkdocs-alabaster">mkdocs-alabaster</a>
      
    </div>
  

  <!--
  MkDocs version      : 1.0.4
  Docs Build Date UTC : 2020-01-15 02:15:26
  -->
</body>
</html>